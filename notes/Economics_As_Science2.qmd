---
title: "The Empirical Pivot: From Laws to Stochastic Models"
author: 'Harrison Youn<br><span style="font-size: 0.8em; color: #6c757d;">Economics as Science 2</span>'
date: "2026-01-06"
description: "How did economists transform the positivist ideal into a disciplined empirical craft?"
image: "/assets/images/Economics_As_Science2.png"
categories: [Economic Methodology, Econometrics, Positive Economics, Haavelmo, Friedman]
from: markdown+tex_math_dollars+tex_math_single_backslash
format:
  html:
    toc: true
    toc-depth: 3
    theme: cosmo
    code-fold: true
    html-math-method: mathjax
---

## Beyond Logic: The Birth of Econometric Ambition

Logical positivism promised a sharp criterion of meaning and an austere picture of science: clarify concepts, purge metaphysics, and test what remains. Its collapse did not leave economists empty-handed. It left them with something more practical and, in the end, more fertile.

If universal economic “laws” cannot be verified in the way the Vienna Circle imagined, what does it mean to justify an economic mechanism empirically?

The twentieth-century answer was neither a single philosophical dictum nor a technical trick. It was an institutional and intellectual invention: **econometrics**, a craft that tried to weld three demands into a coherent workflow:

1. **theoretical structure** that supports counterfactual reasoning,
2. **measurement conventions** that connect theory to observables,
3. **statistical inference** that disciplines belief under uncertainty.

Econometrics is often taught as a toolbox. Historically, it was an epistemic wager: that economics could be made criticizable with rigor even when it could not be made experimental.

![](/assets/images/Economics_As_Science2.png)

The crucial shift was subtle but decisive. Rather than aspiring to read off invariant laws from data, econometrics learned to treat data as the outcome of a **stochastic system**. The question became: under what assumptions does a model constrain what can be observed, and how severely can those constraints be tested?

---

## The Keynes–Tinbergen Controversy: When the Economy Refuses to Be a Laboratory

The first public stress test of econometric ambition was the Keynes–Tinbergen controversy. Tinbergen’s project was audacious: estimate quantitative relations among macroeconomic variables and use them to evaluate theories and policies. Keynes’s critique is remembered for its sharp rhetoric, but the enduring point is methodological.

In the laboratory sciences, causal pathways are isolated by design. In macroeconomics, where “other things equal” is rarely secured by nature, the test of a theory must travel through a chain of auxiliary commitments:

- which variables are included,
- how they are measured,
- whether omitted forces are negligible,
- whether institutions and behavior are stable,
- whether policy alters the very structure being estimated.

Keynes was pressing on the fragile link between statistical fit and scientific warrant. A regression can be numerically impeccable and epistemically hollow if the chain cannot be defended.

A useful way to read Keynes is not as hostility to quantification, but as a demand for clarity about *what exactly is being tested*. This demand never went away. It reappears today whenever we argue about “controls,” “identification,” and the credibility of causal claims.

**When evidence contradicts an economic model, what exactly has failed: the mechanism, the measurement, the environment, or the inference?**

Econometrics, at its best, is the discipline of making this question answerable.

---

## Haavelmo’s Probability Turn: What Makes a Model Empirically Meaningful

Trygve Haavelmo’s *probability approach* is often described as the importation of modern statistics into economics. That description understates the philosophical and methodological force of the move.

Haavelmo redefined what an economic model is, and therefore what it means to test one. A model is not a deterministic sentence written onto the world. A model is a **stochastic data-generating system** that implies a probability law over observables.

A compact template is:

$$
Y = g(X,\theta,U), \qquad U \sim F_U, \qquad (Y,X)\sim P_\theta.
$$

The scientific content of the model is not a story, nor a diagram, nor a verbal mechanism. It is the family of probability laws $\{P_\theta\}_{\theta\in\Theta}$ that the model permits. If the model permits “almost anything,” then it explains almost nothing. Conversely, if it forbids a large class of patterns in the data, it becomes empirically risky in precisely the way a scientific hypothesis should be.

This reframing has two consequences that modern applied economists routinely use, often without naming Haavelmo.

First, **estimation** is a question about how data update beliefs about $\theta$ given the implied $P_\theta$. Whether one speaks the language of likelihood, moments, or Bayesian posteriors, the underlying premise is the same: the model defines a probabilistic map from parameters to observables.

Second, **testing** is no longer a matter of “does the theory sound plausible?” but “does the theory’s implied family $\{P_\theta\}$ survive contact with the empirical distribution?”

These are not cosmetic reinterpretations. They convert “empiricism” from a mood into a logic.

---

## Identification as the Logical Hinge

Haavelmo’s deeper message is that “more data” cannot rescue a model that is not **identified**. Identification is not a statistical nicety. It is a logical precondition for learning.

**Definition (Identification as Non-Observational Equivalence).**  
Let the population distribution of observables be $P$, and let the model imply the set $\{P_\theta:\theta\in\Theta\}$. The parameter $\theta$ is (point) **identified** if

$$
P_\theta = P_{\theta'} \ \Rightarrow\ \theta = \theta'.
$$

If distinct parameters generate the same observable distribution, they are **observationally equivalent**, and no amount of data can logically distinguish them.

This definition is austere on purpose. It strips away algorithmic details and isolates the core idea: identification is about whether the model’s restrictions bind tightly enough to make learning possible.

A further refinement, crucial in practice, is the distinction between **global** and **local** identification. A model may be locally identified near the true parameter (so that small deviations in $\theta$ move $P_\theta$), while still suffering from global pathologies (multiple distant parameter values produce the same distribution). In parametric likelihood models, a classical result links local identification to the nonsingularity of the Fisher information matrix under regularity conditions.

If you want a single sentence that captures why identification sits at the center of modern econometrics, it is this: *statistics begins only after identification has made learning logically feasible*.

---

## A Minimal Identification Parable: Why Equilibrium Data Do Not Reveal a Demand Curve

Consider the textbook market system:

$$
Q^d = \alpha - \beta P + u_d, \qquad \beta>0,
$$

$$
Q^s = \gamma + \delta P + u_s, \qquad \delta>0,
$$

$$
Q^d = Q^s = Q.
$$

Solving for equilibrium price yields:

$$
P = \frac{\alpha-\gamma + (u_d-u_s)}{\beta+\delta}.
$$

If you only observe equilibrium $(P,Q)$, variation in $P$ reflects a mixture of demand and supply shocks. The naive regression of $Q$ on $P$ does not identify $-\beta$. It loads on an endogenous equilibrium object whose movement is itself a compound outcome of two structural disturbances.

Identification arrives only when you can defend variation that shifts one equation while leaving the other unchanged. Suppose you have a cost shifter $Z$ that enters supply but not demand:

$$
Q^s = \gamma + \delta P + \pi Z + u_s, \qquad \pi\neq 0.
$$

Then $Z$ moves $P$ through supply while holding demand fixed, and the demand slope becomes an identifiable object under appropriate exclusion and exogeneity claims.

The methodological point is not the market example. It is the structure:

A model becomes empirically meaningful only when it implies an identifiable mapping from theoretical parameters to observable distributions.

Notice what this does to the practice of empirical work. It elevates certain questions from “robustness checks” to logical requirements:

- What variation identifies the parameter?
- What assumptions secure that variation?
- What observables can refute those assumptions?

This is why good empirical papers often read like an argument in logic, not like a tour of regressions.

---

## From Identification to Inference: Restrictions as Information Content

Once identification is secured, inference becomes meaningful. But the Haavelmo view also clarifies a deeper relationship between econometric restrictions and the “scientific content” of a model.

A restriction is informative precisely because it rules out data patterns. Exclusion restrictions, functional forms, monotonicity, independence assumptions, and timing assumptions each carve away parts of the space of possible joint distributions of observables.

Some restrictions are directly testable (at least partially). Overidentifying restrictions in instrumental-variable settings are a familiar example: if you have multiple instruments, the model implies multiple ways of “explaining” the same variation, and the implied coherence can be probed.

Other restrictions are not directly testable in finite samples but still have indirect empirical footprints. For example, a structural model used for counterfactual policy evaluation implicitly claims a kind of stability: the parameters that summarize behavior will remain invariant under certain interventions.

This brings us to a central methodological fork in economics.

---

## Structural Versus Reduced Form: What Econometrics Is Really Choosing Between

Haavelmo’s framework clarifies the distinction that later dominates econometric practice.

A **structural** model targets parameters with mechanistic interpretation, intended to travel into counterfactual worlds. Structural claims demand defended invariance: the mapping from preferences, constraints, and technology to observables should remain stable under the counterfactual intervention.

A **reduced-form** relation targets stable predictive regularities, often with fewer commitments about underlying mechanisms. Reduced-form claims demand defended stability over the environments in which the relation is to be used.

Neither approach is automatically superior. Each carries its own burden of justification. What changes is the location of the burden. Structural work pays upfront in assumptions to buy interpretability and counterfactual reach. Reduced-form work pays in scope limits, insisting on a disciplined modesty about what the estimates mean beyond the observed environment.

This is not merely a philosophical distinction. It becomes a practical one whenever policy changes behavior, which is why the Lucas critique later became a methodological watershed for macroeconometrics.

---

## Measurement Without Theory, and Theory Without Measurement

The next methodological conflict was internal to empirical work itself.

On one side stood the NBER tradition: careful measurement, business-cycle dating, and the accumulation of descriptive regularities. On the other side stood the Cowles tradition: structural modeling, simultaneous equations, and inference guided by explicit theory.

Koopmans’s charge, often paraphrased as “measurement without theory,” is best read as a claim about scientific cumulation. Without theoretical scaffolding, measurement risks becoming an archive of correlations that does not converge on explanation. But the counter-charge is equally serious: theory without credible measurement risks becoming a closed symbolic exercise, insulated from what the data can sustain.

Modern empirical economics still lives in this tension. The field’s best moments often occur when the tension becomes productive, when careful descriptive work locates durable patterns that structural work can then attempt to explain, and when structural work generates sharp empirical predictions that force measurement to become more precise.

---

## Friedman’s “As If” Defence: A Re-Ranking of Scientific Virtues

Milton Friedman’s methodological essay shifted attention from the realism of assumptions to the performance of a model’s **testable implications**. It is often caricatured as “assumptions can be false.” The stronger reading is methodological:

- models are not evaluated as literal descriptions of cognition,
- models are evaluated by whether their implications survive discriminating tests,
- predictive success is a form of methodological selection under criticism.

If one wants to make Friedman’s position more rigorous, the key is to connect “as if” to **information content**. A model earns scientific standing not by narrating the true inner workings of agents, but by compressing the world into a small set of assumptions that generate a large set of nontrivial implications. Predictive accuracy then becomes evidence that the compression preserved the relevant structure.

Under this reading, a good model does not merely fit. It fits *for the right reasons*, where “right reasons” means: it continues to perform when the test is made harder, when the data environment shifts within the model’s domain, when alternative models are forced to compete under the same predictive criterion.

### Untwisting the F-twist

Later philosophy of economics sharpened the debate by refusing to treat “unrealistic assumptions” as a single category. Some assumptions are:

- **negligibility** assumptions, setting small forces to zero,
- **domain** assumptions, restricting scope rather than describing the world,
- **heuristic** assumptions, scaffolding that supports exploration and then is replaced.

Conflating these makes methodological disputes look like taste. Distinguishing them makes criticism precise. It also explains why economists can rationally keep some “unrealistic” assumptions while rejecting others.

---

## The Boundary Problem Returns: What Prevents Curve Fitting?

If a model is a stochastic generator, and if assumptions are not literal descriptions, why is econometrics not merely curve fitting?

The best answer is not a slogan. It is a practice: increase the difficulty of survival.

- defend identification, not just statistical significance,
- separate exploration from evaluation when possible,
- use out-of-sample validation when prediction is the goal,
- use placebo tests and falsification checks when causal interpretation is the goal,
- demand robustness that targets plausible failure modes rather than cosmetic variations.

This is a pivot away from verification as certainty and toward criticism as discipline. If Part 1 ended with the collapse of verifiability, Part 2 ends with the emergence of a different ideal: scientific knowledge as what endures severe attempts at refutation.

To make that ideal explicit, we need the next philosophical move. We need a logic of risk.

That is where Popper enters.

---

## References

- Haavelmo, T. (1944). *The Probability Approach in Econometrics* (Cowles Foundation Paper No. 4). 
- Keynes, J. M. (1939). “Professor Tinbergen’s Method.” PDF: 
- Koopmans, T. C. (1947). “Measurement Without Theory.” PDF: 
- Friedman, M. (1953). “The Methodology of Positive Economics,” in *Essays in Positive Economics*. 
- Rothenberg, T. J. (1971). “Identification in Parametric Models.” 
- Mäki, U. (2000). “Kinds of Assumptions and Their Truth: Shaking an Untwisted F-Twist.” 
- Lucas, R. E. Jr. (1976). “Econometric Policy Evaluation: A Critique.”
